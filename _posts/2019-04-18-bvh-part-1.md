# Path tracing millions of spheres in CUDA

## Motivation of this post

In the previous post series [**link to it here**] our path tracer could only handle scenes that could fit in constant memory. In this post we are going to investigate what it takes to render millions of objects. To do that we'll have to store the scene in global memory and use some acceleration structure to improve the performance of the renderer.

We want to focus on the ray-scene intersection and to do so we are going to change the renderer to only support spheres of radius 1 with a single Lambertian material used by all. This will help us focus our performance analysis on the ray-scene intersection.

Coming up with interesting scenes that only contain spheres can be tricky, but luckily Fogeleman [**correct his name and link to his tweeter account**] has a nice DLA generator that can generate pretty interesting scenes with as many spheres as we want [**link to his blog post**].

## Preparation

We are going to start from the path tracer we wrote in the previous post series and update it to only support spheres, a single Lambertian material, and to load the whole scene in global memory [**link to the commit**].

Fogelman's DLA generator can generate as many spheres as we need, we are going to focuson 1M and 10M scenes in our tests. Loading the scenes is as simple as reading a csv file, my code is based on the following [blog post](https://waterprogramming.wordpress.com/2017/08/20/reading-csv-files-in-c/).

Rendering 1 million spheres at 1200x800 and 1 sample per pixel (spp) took 594s. The renderer was not able to render 10 million spheres as it kept crashing.

## BVH

Instead of intersecting each ray with the spheres in the scene, we could build a Bounding Volume Hierarchy (BVH). The general idea is similar to how binary sort accelerates search in a sorted list: Instead of intersecting the ray with all the spheres, we subdivide the scenes into a hierarchy of bounding boxes and only iterate through the nodes that intersect with the ray [**write a better explanation about BVH**]. Peter Shirley's [Raytracing the next week](http://www.realtimerendering.com/raytracing/Ray%20Tracing_%20The%20Next%20Week.pdf) has a detailed chapter that explains how to build and use BVH to accelerate ray-scene intersection.

To keep things simple, I decided to build the BVH on the host and just copy it as is to the device. The building logic is not different from the book except for the memory representation of the nodes: instead of storing pointers to the left and right children, I store all the nodes in a heap array using the following convention:

recursive implementation based off Peter Shirley's (blog post)[http://psgraphics.blogspot.com/2019/03/making-your-bvh-faster.html]
modified to access the nodes array as follows:

- root node is at nodes[1]
- each node nodes[idx] has 2 children nodes[idx*2] and nodes[idx*2+1]
- each node nodes[idx] has its parent at nodes[idx/2]

For a scene of N spheres, and assuming for simplification that N is a multiple of 2, we need a BVH tree with a total of N-1 nodes.
Each of the N/2 leaf nodes wraps 2 spheres. We store the bvh nodes and spheres in separate spheres and we can use the node_idx to identify
if its refering to a bvh_node (node_idx < N) or a sphere (node_idx >= N).
Assuming we have the following struct:

```
struct scene {
  bvh_node *nodes;
  sphere *spheres;
  uint count; // N
};
```

Traversing the BVH tree is done as follows:

```
bool hit_bvh(const ray& r, const scene& sc, uint node_idx, float t_min, float t_max, hit_record& rec) {
  if (node_idx >= sc.count) {
    return hit_sphere(r, sc.spheres[node_idx-sc.count], t_min, t_max, rec);
  }

  if (hit_bbox(r, sc.nodes[node_idx], t_min, t_max)) {
    if (hit_bvh(r, sc, node_idx*2, t_min, t_max, rec)) {
      hit_bvh(r, sc, node_idx*2+1, rec.t, t_max, rec);
      return true;
    }
    return hit_bvh(r, sc, node_idx*2+1, t_min, t_max, rec);
  }
  return false;
}
```

Recursive implementation doesn't work well on device as it requires a stack propertional to the depth of the BVH tree and quickly runs out
of memory. Iterative implementation is also straightforward:

```
bool hit_bvh(const ray&r, const scene& sc, float t_min, float t_max, hit_record& rec) {
  bool down = true;
  uint node_idx = 1; // root node
  bool found = false;
  float closest = t_max;
  
  while (true) {
    if (down) {
      if (node_idx >= sc.count) { // this is a leaf (sphere) node
        if (hit_sphere(r, sc.spheres[node_idx-sc.count], t_min, closest, rec)) {
          found = true;
          closest = rec.t;
        }
        down = false;
      } else {
        if (hit_bbox(r, sc.nodes[node_idx], t_min, closest, rec)) {
          node_idx *= 2; // go to left child
        } else {
          down = false;
        }
    } else if (node_idx == 1) {
      break; // we backtracked to the root node
    } else if ((node_idx%2) == 0) { // node if left
      node_idx++; // go to right sibling
      down = true;
    } else {
      node_idx /= 2; // node = node.parent   
    }
  }

  return found;
}
```

## Results

for a scene of 1M spheres at 1200x800 and 1spp, rendering time went down from 594s to 2s. And a scene of 10M spheres that used to fail is now rendering in 5.5s. 
Building the BVH takes time though, +18s for 10M spheres.

Using nvprof we can collect a few metrics that can help us confirm what is happening in the kernel. For a scene of 8K spheres we get the following numbers:

| Metric | Description | Without BVH | With BVH |
|--------|-------------|-------------|----------|
| dram_read_transactions | Device Memory Read Transactions | 12,587 | 13,077 |
| gld_transactions | Global Load Transactions | 18,733,000,000 | 924,087,362 |

It's interesting to see that we 20x less global load transactions because the rays require less data to traverse the whole tree, yet we are still submitting around the same
number of memory read transactions most likely because there is less memory coalescing between same warp threads.
Even though the kernel is still reading about the same volume of data from device memory, the huge performance boost we get is probably because the ratio of processing vs waiting on
memory reads increased thus allowing the device to get more done while waiting for the data to come back from memory (latency hiding).
